%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Grundlagen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Material and Methods}
\label{matmet}

- Material: Was ich für ein Datensatz zum Testen benutzt habe und wo der herkommt
\label{lab:matmet:dataset}

\section{Implementation of the percolator algorithm}
- Wichtige Punkte wären:\\
- Verwendete Scipy-Methoden\\
- Abbruch wenn es nicht besser wird und dass ich die AUC als Metrik nutze\\
- \label{lab:matmet:normalization} feature normalization\\
- \label{lab:matmet:pseudoROC}Wichtige Hilfsfunktionen (pseudoROC zB)\\
- Heißt jetzt Pycolator

\section{Adapting Percolator to Cross-link Identification}
To be able to monitor the difference any experiment makes, especially with respect to the cross-linked or non-cross-linked PSMs, following features were implemented:\\
First, in addition to the q-value, which is calculated as described in~\ref{background}, the calculation of a class-specific q-value was implemented. This is done by splitting the dataset according to the class affiliation and calculating the q-value separately for both splits. This allows a more precise performance estimation when looking at only one of the classes.\\
\label{lab:matmet:rocs_after_every_iteration}
Second, a ROC curve using the \emph{pseudoROC} function~(\ref{lab:matmet:pseudoROC}) is plotted after every iteration of Pycolator: one for the whole dataset, one only for cross-linked, and one only for non-cross-linked PSMs. Accordingly, the respective class-specific q-value is used. Thus, three plots covering the corresponding class(es) and every iteration, as well as the area under the curve are output. This allows for fast visual detection of the impact a specific change to the algorithm has on certain classes, iterations or general sensitivity.
\subsection{Different Ranks}
\label{lab:matmet:ranks}
As experience shows, cross-linked peptides can be harder to detect than linear peptides. This means, the possibly correct cross-linked peptide will frequently not get the highest score of all the peptides. It thus can be beneficial to not only consider the highest scoring peptide, but also the highest scoring cross-linked peptide or also some lower-scoring peptides, and assign them ranks. Then, as following experiments show, Percolator can correct the scores of some lower-ranking PSMs, possibly detecting more cross-linked PSMs. Meanwhile it is known to the experimenter, that only one of the peptides can be the correct match to a spectrum, and thus any PSM with a lower rank than 1 should be excluded at the end.\\
To tackle both constraints, Pycolator first trains the SVM with every PSM available and re-calculates the ranks based upon the newly assigned score after every iteration, possibly correcting the ranks of some PSMs. When the used metric, normally the area under the pseudo ROC curve, does not improve beyond a certain threshold per iteration, every PSM with rank $2+$ is dropped. The threshold is controlled by the parameter \texttt{cutOffImprove} with a default of $0.01$ corresponding to a $1\%$ increase of the used metric per iteration. Since also some of the best scoring PSMs will be dropped (even though they were not assigned rank 1), the algorithm then runs further until the maximum iterations are reached or the score does not further improve, in order to properly integrate and score the new PSMs considered confident.\\
The performance of this feature was tested against dropping the lower ranking PSMs once at the very end of the algorithm and once at the very beginning. Pycolator was run on the given dataset~(\ref{lab:matmet:dataset}) and pseudo ROCs were plotted as described in~\ref{lab:matmet:rocs_after_every_iteration}.
\subsection{Characteristics of Cross-linked PSMs}
Apart from being hard to detect, cross-linked peptides also have other characteristics, some of which pose problems to the computational detection of correct PSMs. As discussed in~\ref{lab:matmet:splitting}~"Splitting the dataset", the features of cross-linked and linear peptides are so dissimilar, splitting the dataset and training a linear SVM separately on cross-links and non-cross-links yields significantly better results than training one linear SVM. To reduce the impact of this heterogeneity, the following experiments were conducted:
\subsubsection{Proportions of Different Classes}
\label{lab:matmet:proportions}
As discussed in~\ref{lab:background:percolator}, Percolator employs a nested cross-validation approach, splitting the dataset, training on all parts than one and testing/scoring on the remaining part. If the splitting was uneven by chance, the SVM would be trained badly and the scoring inaccurate. Having different classes with significant differences in the dataset, like in our case cross-linked and linear PSMs or targets and decoys, increases this problem. For example, if there was a testing split with many cross-linked PSMs, the SVM had to be trained on the remaining data with few cross-linked PSMs, resulting in probably poor scoring of the many cross-linked PSMs in the test set. In the average case, this should not be a problem, but it can occasionally produce worse or better results, which would follow from overfitting.\\
To solve this problem, a mechanism of maintaining the proportion of the classes in the whole dataset for every inner and outer split was implemented. It can be toggled for targets and decoys or cross-linked and non-cross-linked PSMs as well as for inner and outer split independently. The impact has been measured by running the algorithm $10$ times, plotting and recording the results of the best, worst and median run w.r.t. to the q-value any number of PSMs was identified at. Because this took approximately 90 minutes, the test was performed in a Google Colaboratory notebook\footnote{The Google Colaboratory notebook used:\\ https://colab.research.google.com/drive/1VqZAmdta57YhgobA0WkQMqe\_U9YIUnDl?usp=sharing}.
%\begin{figure}
%	\label{fig:fluctuation_of_percolator}
%	\centering
%	\includegraphics[width = 0.7\textwidth]{figures/percolator_MaxMinMedian_ClassesOption=.png}
%	\caption[Fluctuation of an earlier version of Percolator]{Pseudo ROC curve of an earlier version of the Percolator algorithm. It was run $10$ times and the best, worst and median run were plotted. Although the best run is close to the median one, the worst run is off by almost $1000$ PSMs. }
%\end{figure}
\subsubsection{Imputation}
\label{lab:matmet:imputation}
Cross-linked PSMs naturally have features linear PSMs do not have, which can however be used for training the SVM. An example would be the nucleotide it was linked to, or the partial loss score, which is the X!~Tandem~\cite{Craig2004} hyper score for the cross-linked peaks. In the given dataset~(\ref{lab:matmet:dataset}), $16$ out of $61$ features were only given for cross-linked PSMs. Optimally, these should not influence the score a non-cross-linked PSM gets. However, $0$ was filled in for the missing values and because that is a valid value for the linear SVM, it biases the decision made. For example, if a high value in a feature leads the SVM to a decision against the PSM, $0$ as the lowest value possible after feature normalization~(\ref{lab:matmet:normalization}) will tell the SVM to give the PSM a higher score. However, the scikit-learn package provides solutions for this problem\footnote{https://scikit-learn.org/stable/modules/impute.html}, and one of these, the \texttt{IterativeImputer}, was tested. % with Pycolator and the dataset~\ref{lab:matmet:dataset}
\subsubsection{Splitting the Dataset}
\label{lab:matmet:splitting}
Because the SVM is linear, one feature can not alter the influence another feature has on the decision. Thus, splitting the dataset into cross-linked and non-cross-linked PSMs should allow the SVM to fit more precisely onto the characteristics of each. It was also tested splitting the dataset by the nucleotide the peptide was cross-linked onto, and non-cross-linked PSMs as a fifth class. As will be discussed later, neither is applicable to most datasets, but it gives an upper limit for how well the algorithm could perform on a joint dataset. 
\subsection{Small datasets}
\label{lab:matmet:small_datasets}
The portion of cross-linked peptides in a dataset is often very small. To evaluate when splitting as proposed in~\ref{lab:matmet:splitting}~"Splitting the Dataset" is possible and obtain general insight into the scalability of Pycolator, two experiments were conducted to conclude how small a dataset may be, so that the Percolator algorithm still works. In each case, Pycolator was applied to a dataset sampled from the given one~(\ref{lab:matmet:dataset}). The area under the pseudo ROC curve and the number of PSMs identified at $1\%$ q-value were recorded and used as metrics. The results achieved by Pycolator were compared to the effect the splitting had on the dataset utilizing the given, precomputed NuXL score.\\
First, the smaller dataset was sampled using every $2^i$-th PSM, with $i$ being a natural number up to 13, from the whole dataset. Second, the smaller dataset was sampled randomly using a uniform distribution from all PSMs with a q-value of $<10\%$. The portions sampled were $\frac{1}{2^i} : i\in\mathbb{N} \land i\in[0,12]$. Additionally to the metrics mentioned above, also the expected number of identifications at $1\%$ q-value for the split were recorded. For the NuXL score as reference and the Pycolator score respectively one plot showing the portion of PSMs with a q-value of $1\%$ and one plot showing the ratio of found to number of expected PSMs with $1\%$ q-value as a function of the dataset size were drawn. In the end, the ratio of AUCs and identifications at $1\%$ q-value generated by the NuXL vs Pycolator score were plotted as a function of the dataset size. To compensate for the randomness in this experiment, Monte-Carlo-Sampling with 10 iterations was performed and the results were presented with boxplots. \\
Because numerical problems occur when calculating the area under a pseudo ROC curve of a very small dataset (since the curve is only one point), another metric was introduced: the number of identified PSMs at a q-value threshold of $1\%$. Pycolator decides automatically if this metric is required based upon the size of the dataset, and adjusts its log, plots per iteration and functions like the early stopping criterion accordingly.\\\\
(- Performance auf anderem Datensatz\\
- Vergleich mit Entrapment FDR)