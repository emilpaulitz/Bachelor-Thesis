%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Diskussion und Ausblick
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}
\label{discussion}
As already explained in~\ref{lab:matmet:ranks}, cross-linked peptides often are harder to score than linear ones. Therefore, they can get a lower score than appropriate and it can be useful to also include the best scoring cross-linked peptide when running the Percolator algorithm. It may be able to revise the PSMs scores and the actually correct cross-linked peptide may become the best scoring PSM. This thesis is also supported by the findings in~\ref{lab:results:ranks}. If only including the best scoring peptide, of which~\ref{fig:only_rank_one} shows the results, the end result is worse than when giving Pycolator some iterations to re-rank the found PSMs. However, it was better than when giving Pycolator all of the PSMs available, which is unexpected, since giving a machine learning model more information should generally improve its learning. Apparently, the lower ranking PSMs, even when having such a high score a q-value of $\leq5\%$ is estimated, contain misleading information and thus the SVM learns patterns not valid for correct PSMs. This also explains why the algorithm converges faster when only given rank 1 PSMs. The higher quality of data lets the SVM learn the correct patterns after fewer iterations. \\
The pseudo ROC generated when using the newly implemented mechanism~(\ref{fig:optimalranking}) shows a convergence after $5$ iterations, just like when using every PSM~(\ref{fig:all_ranks}). Then, as the log shows, lower ranking PSMs are dropped and the next iteration has a much higher AUC, probably as a result of the better quality of the PSMs. Letting the algorithm run on the new dataset again improves the AUC even beyond that of Pycolator when only using rank 1 PSMs. This suggests, that indeed a re-ranking takes place in the first half of iterations.\\
Comparing the AUC of Pycolator when run with the new mechanism~(\ref{fig:optimalranking}) after iteration $6~(345.26$) with the end result of running Pycolator with every PSM available~(\ref{fig:all_ranks},~$343.21$), yields the following insight: Dropping all PSMs with rank $2+$ yields a worse result when the Percolator algorithm has been running for 10 rather than 6 iterations. This implies an overfitting onto the PSMs with lower quality and thus a worse scoring.\\\\
Although implementing the balancing of different classes in the nested cross-validation splits had no big impact, it reduced the spread. On other datasets, containing much fewer cross-linked than non-cross-linked PSMs, this procedure could have greater impact. This needs to be tested in further experiments. The feature has no disadvantages except negligible worse performance and can be generalized to other classes that could be present in a dataset.\\
Using imputation did not improve the performance of Pycolator. Neither for a single class nor for both. It is thinkable that imputation has a different effect on other datasets, which contain far fewer cross-links than non-cross-links. It could then prevent that the SVM fits onto the characteristics of non-cross-links because they are more frequent and find very few cross-links, but rather fit onto both classes in the same way. This has to be tested in further experiments. Because the imputation takes a significant amount of time, this option is switched off per default.\\
Splitting the dataset by cross-linked and non-cross-linked PSMs slightly improved the performance of Pycolator, and splitting by the cross-linked nucleotide yielded a great improvement. In practice, neither is applicable to most datasets, because they often only contain a very small number of cross-linked PSMs. As the experiments in section~\ref{lab:results:small_datasets} show, applying Pycolator on a small dataset bring a lot of variation.\\
The non-random approach in the first experiment regarding small datasets keeps the ratio of correct to incorrect PSMs and ensures the appearance of PSMs originally classified as correct. However, maintaining the ratio when sampling a very small subset means it will contain one or two PSMs, which originally were assigned a q-value of $<1\%$, and many worse PSMs. This general bad quality of the dataset makes it difficult to obtain quantitative results for the very small datasets, which is why the second experiment was conducted. The findings however do suggest that down to a dataset size of about 500, Pycolator does not suffer from any drawbacks. With smaller datasets, the AUC metric does not work anymore and a lot of variation happens~(\ref{fig:results:small_dataset_first_auc_ratio_pxl}). Only the first problem could be solved: by implementing the new metric of identifications at $1\%$~q-value. Figure~\ref{fig:results:small_dataset_first_ratio_dxl} was generated to test if the FDR estimation would suffer from small datasets. This can not be observed, as the smallest dataset for which a point is plotted has approximately $100$ PSMs, and thus the small increase from $0.061$ to $0.07$ is due to rounding.\\
In the second experiment however, this thesis seems to be supported by figure~\ref{fig:results:small_dataset_snd_found_dxl}, showing that when re-calculating the q-value after sampling, without changing the score, more PSMs receive a high confidence. But this is likely due to the fact that few high-scoring decoys determine the q-value of many high-scoring targets. When one samples randomly, the chance that many of those decoys will not be sampled increases with a smaller sampling size, enhancing the FDR estimation of the targets, which usually lie above $1\%$~q-value.\\
As figure~\ref{fig:results:small_dataset_snd_comparison} shows, with the whole dataset the Pycolator~score achieves a significantly better separation of true and false PSMs than the NuXL~score. However, the smaller the given dataset gets, the smaller this advantage becomes, rarely Pycolator performs even worse.

- Ist Pycolator besser geworden?\\
- Neues Diagramm zeichnen\\
- Methoden hinterfragen oder begründen, Ergebnisse interpretieren, Anwendbarkeit diskutieren, z.B.:\\
- Falsche Formel für q-value\\
- C Parameter für jeden split neu optimieren führt zu overfitting? $\rightarrow$ Original-Algorithmus macht es auch so\\
- Wie sinnvoll ist die neue Metrik (idents bei 1\%)?\\
- ScanNr Versuche: Gleiche Spektren (identifiziert anhand der ScanNr.) auf verschiedene splits verteilen verändert nichts, d.h. vermutlich sind die niedrigeren Ränge dann so schlecht, dass es nichts bringt die schonmal gesehen zu haben.\\
- Peptide Versuche: Schlechtere Ergebnisse, aber vllt ehrlicher?\\
%Die Generalisierung von manchen Peptiden auf alle Pepide fällt der SVM schwer, weil der score sinkt wenn man die Peptide im test set nicht auch im train set zeigt.\\
%auf was soll man aber noch generalisieren, die svm wird gelöscht sobald der split fertig ist. Ist es hier unehrlich oder besseres training, wenn man der svm gute Beispiele zeigt?\\		
%Idee: Vielleicht sind es nur wenige, bestimmte Peptide (oder Proteine), die besondere Eigenschaften haben und somit schlecht vorhergesagt werden können?\\
%Oder: Peptide von decoys und peptide von targets sind disjunkt. Vielleicht haben gleiche Peptide gleiche Eigenschaften in manchen der scores, und die SVM kriegt diese Eigenschaften über die false train mit? Und kann somit zwischen decoy und target direkt unterscheiden?\\
%$\rightarrow$ weitere Experimente? Zu beachten: nXL werden hier immer schlechter!\\
\section{Outlook}
- Mögliche weiterführende Experimente: mächtigere Klassifikatoren + monotonic constraints (wie von Timo ausprobiert), Ada-Boosting, feature selection
