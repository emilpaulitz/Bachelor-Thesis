PSM = Peptide Spectrum Matches
# Frage: Warum generiert man nicht mehrere decoy-databases (shuffled, reversed...), damit nicht jeder zweite falsche Eintrag ein decoy ist, sondern jeder dritte, vierte, fünfte....?
# Antwort: Sehr sehr aufwändig, besonders bei cross-links. Wenn ich ne Idee habe kann ich die Daten dazu bekommen. Viele decoys bedeuten dass auch hohe scores random generiert werden.
# Frage: Die Aufteilung auf 3 subsets für cross-validation nur mit PSMs, die eh fürs Training verwendet werden oder mit allen und dann werden daraus die benötigten PSMs ausgewählt (momentan implementiert)?
# Antwort: Gute Frage, überlegen wir uns nochmal. Überlegung wert, gleichmäßiger samplen oä
# Frage: Warum kommt immer ein anderes Ergebnis, wenn man öfter die Funktion decision_function anwendet ohne neu zu fitten? Werden den PSMs nicht immer die gleichen scores gegeben? Code:
df['test_score'] = W.decision_function(X)
calcQ(df,'test_score')
print(len(df[df['q-val'] <= 0.01]))

# Frage: Was bringt normalisierung nach unit norm? -> Ziemliche Verbesserung der Trennleistung zumindest
Numerische Gründe
optimierung probleme mit floating point
kleine zahlen werden weggerundet -> man sieht die unterschiede nicht mehr

# Welche mächtigeren Klassifikatoren wären sinnvoll um die monotonic constraints zu verwenden? Auch non-linear svm? -> später 
# Idee: trainingsteil von df so bewerten wie ich es gerade tue, aber rest mit einem klassifikator, der auf alles Daten trainiert hat? 
	# für normalisierung brauche ich wahrscheinlich repräsentativen Ausschnitt aus den Daten, oder?
	# Das ist beim rest nicht möglich, da ich einen bestimmten Teil komplett fürs Lernen verbraucht habe.
# Was ist eigentlich mein Ziel gerade? Dass es möglichst gut funktioniert, und dann auf Daten anwenden die Timo mir noch schickt. Was schafft der C-percolator? -> Etwa so wie meine Implementierung auch
# evalXL(p) vs evalXL(dSlow) zeigt, dass non-XL deutlich weniger verbessert werden als XLs. Liegt das daran, dass der NuXL-Score schon besser mit non-XLs umgehen kann? Ja vermutlich (?)

# Frage: Ist es schlimm dass es immer viel mehr negative als positive Beispiele gibt? (ca 10:1) Könnte Grund sein, dass nur Rang 1 besseres Ergebnis erzielt? -> weniger Decoys, Verhältnis etwa 2:1
	Habe zum Test mal angeboten, dass C- auch weniger sein darf als C+, und er hat das teilweise angenommen. Bedeutung?
	"balanced"-Option wurde aber nicht genommen...
# Frage: Kleiner Unterschied zw. Percolator und mir: Bei gridsearch nimmt percolator die Werte, die die meisten PSMs bei bestimmtem q geben. Ich nehme die, die besten Wert bei Training geben. Schlimm?
# Frage: Im Datensatz sind XL:nXL 5:4, aber wenn man nur Rang 1 betrachtet ist es 4:1 ??
	Vielleicht einfacher, XLs zu verbessern als nXLs, ist bisher praktisch immer so. Könnte das ein Grund sein für die bessere Performance wenn man nur Rang 1 nimmt?

# Nachschauen: https://pandas.pydata.org/pandas-docs/version/0.15.2/indexing.html
Combine DataFrame’s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:

In [116]: values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}

In [117]: row_mask = df.isin(values).all(1)

In [118]: df[row_mask]
Out[118]: 
  ids ids2  vals
0   a    a     1

# Code Schnipsel: 
falseTrain, trueTrain = percSelectTrain(df, qTrain, rankOption, lowRankDecoy, idColName)
        train = falseTrain[features].values.tolist() + trueTrain[features].values.tolist()
        classes = [0] * len(falseTrain) + [1] * len(trueTrain)
        
        for ixTrain, ixTest in LeaveOneGroupOut().split(train, classes, groups = splits):
            
            validate = df.loc[ixTest, features]
            training = df.loc[ixTrain, features]
            
            #test
            parameters = {'C':[0.1,1,10], 'class_weight':[{0:i, 1:1} for i in [1,3,10]]}
            if(balancedOption):
                parameters['class_weight'] += ['balanced']
            W = svm.LinearSVC(dual = False)

            groups = training['Peptide'].tolist()
            clf = GridSearchCV(W, parameters, cv = GroupKFold(n_splits=3).split(training, classes[ixTrain], groups))
        


1.	Mit Art der Daten vertraut machen, parsen, FDR und q-value berechnen, Pseudo-ROCs zeichnen (Ordner 'Erste Aufgabe')
2.	Richtige Daten mit XL bekommen, Skript aus 'Erste Aufgabe' anwenden, Pseudo-ROCs für XL und nicht-XL zeichnen.
3.	GitHub anlegen
4.	Rang-Spalte hinzufügen und jeweils nur Rang 1 für ROCs benutzen
a.	Vorher (alle Einträge mit entspr. Q-Value, fast method, orange: XL, blue: non-XL): ROC_fast_all.jpg
b.	Nachher (nur Rang-1-Einträge, fast method, orange: XL, blue: non-XL): ROC_fast_rank1only.jpg
c. 	Non-cross-links sehen gleich aus, Cross-links haben gleiche Form aber Anzahl ist bei allen Ranks bis q = 0.05 bei ca. 6000, nur bei Rank 1 nur ca. 4000
5.	Einlesen in lineare ML-Ansätze und ML-package scikit Learn
a. 	LDA: Geht von Normalverteilungen mit unterschiedlichem Erwartungswert und/oder Varianz der einzelnen Scores für jede Klasse aus. Das passt nicht wirklich auf unser Beispiel, da (zumindest die meisten) scores nicht normalverteilt sind (oder?). Ergebnis: lda_compared_to_NuXLscore.png
b. 	Ladder score ist etwas normalverteilt, mit einem deutlich unterschiedlichen Erwartungswert zwischen den wahren und den falschen Trainigsdaten. Nur den zu verwenden macht das Training (natürlich?) nicht besser, wieviel schlechter es ist müsste man noch mehr rausfinden. Ergebnis: lda_of_ladder_score.png
c.	Logistic regression: Trennt ähnlich wie NuXL score. Warum? Wie wird NuXL score zusammengesetzt? Ergebnis: lr_compared_to_NuXLscore.png
d.	SVM: Trennt ähnlich wie NuXL score, wird von percolator benutzt, versucht die Grenze so zu legen, dass der Abstand der Grenze zu Objekten der Klasse maximal ist, ohne zu viele zu wichtige Datenpunkte als Ausreißer zu klassifizieren. Ergebnis: svm_compared_to_NuXLscore.png
6.	Struktur des Notebook verändern, dass man die Module beliebig zusammensetzen kann
7.	Percolator-Algorithmus nachbauen 
Notes:	Seltsames Verhalten der Scores der SVM, siehe Linear_ML.ipynb letzte Zelle. Vielleicht müssen die Scores nochmal transformiert werden? Darum benutze ich allerdings predict_proba bzw. eine lineare SVM, bei der gibt es die Probleme nicht.
	Option 'dual = False' hat es signifikant schneller werden lassen, sodass das Lernen fertig wird (konvergiert) und die Ergebnisse tatsächlich besser sind als der NuXL:score
	Auch die Pseudo-ROC-Kurven wenn man nach dem NuXL:score geht sehen unterschiedlich aus, da im Percolator-Algorithmus zufällig die Hälfte der Decoys entfernt wird.
	Experiment:
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = : 5569
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = balanced: 5560
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = : 5351
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = balanced: 5185
		
		Genauere Untersuchung zu den Klassen Optionen mit den Eckwerten von je 10 Ausführungen (q <= 0.01, rank = 1):
		class Option: , max PSMs: 5285, min PSMs: 4413, median PSMs: 5203.0
		class Option: balanced, max PSMs: 5261, min PSMs: 5067, median PSMs: 5179.0

		Genauere Untersuchung dazu, nur Rang 1 fürs Training zu benutzen, zum Scoring aber alle zu nehmen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5111, min PSMs: 4876, median PSMs: 4987.5
		useRankOneOnly: False, max PSMs: 5333, min PSMs: 4320, median PSMs: 4956.0

		Genauere Untersuchung dazu, nur Rang 1 PSMs für Training und Scoring zu benutzen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5669, min PSMs: 5011, median PSMs: 5569.0
		useRankOneOnly: False, max PSMs: 5380, min PSMs: 4364, median PSMs: 5238.0
		Mögliche Gründe: PSMs mit q <= 0.01 aber Rang > 1 (also eher schlecht) werden ausgeschlossen. Es gibt weniger schlechte PSMs im df die man falsch klassifizieren kann?

		Vergleich: mit NuXL:score mit q <= 0.01 findet man 4067 PSMs.
		Bilder jeweils im 'results' Ordner.
8.	Cross-Validation einbauen
9.	XL-q-value berechnen: Q values nur mit den klassenmembern berechnen (extra spalte).
		Iterationen-plots für XL und nicht XL getrennt. 3 plots: XL, nicht-XL, beides zusammen, plus AUC in Legende
		für allgmeine ROC kurven auch klassen q values benutzen
10. 	Wenn es in den letzten 4 (standard) Iterationen schlechter wird, wird letztes bestes genommen
11.	features nach unit norm normalisieren. d.h. min jedes features wird 0 und maximum wird 1.
		Ergebnisse werden viel besser! Bilder ohne normierung: percolator_acv_{}_noNorming.png, mit normierung z.B. percolator_acv_{}.png
12.	Experimente:
	Normal: percolator_acv_{}.png
	Ränge mit einbeziehen:
		Wenn auf Rang 1 target : target, wenn auf rang 1 decoy, alle decoy? "RankOption = True": uses all PSMs of species, of which the first-ranked PSM is a decoy, as negative training examples, and no longer as positive training examples.
		Ergebnisse bleiben etwa gleich. decoy trainingsbeispiele: 42592 decoys + 11200 = 53792, weniger pos. Trainingsbeispiele: 5498 - 18 = 5480. Bilder: percolator_acv_{}_rankOptionTrue.png
		
		Während training Rang 2+ target als decoy behandeln -> Jede Iteration schlechter, ob bei pos train set nur Rang 1 betrachtet wurde (Bilder: percolator_acv_rankTest_{}.png) oder nicht.
		zum scoring aber immer wieder wie original!
		
		!!!AB HIER: rankOption = True neues normal!!!
	Generell nur Rang 1 benutzen:
		Bilder: percolator_acv_{}_useRankOneOnly.png
		Bestes Ergebnis bisher. Schlechtere Ränge verwirren percolator? Endergebnis ist auch besser als normal und ganz am Ende nur Rang 1 zurückgeben (wird immer gemacht) (percolator_acv_ROC_normal.png)

	ScanNr Versuche:
		Sehr ähnliche Peptide auf verschiedene splits verteilen führt zu gleichen PSMs in training und test set. Abhilfe: nach scanNr sortieren und dann splitten (nicht mehr random, abhilfe: gleiche scanNr beim aufteilen als 1 Element behandeln -> alle gleichen scanNr kommen in gleichen Split)
			Momentan noch hässlich implementiert mit immer neuen DataFrames -> müsste man ändern. 
			Ergebnisse nicht viel anders als sonst. Bilder: percolator_acv_{}_ScanNrTest.png
			Vermutlich sind die niedrigeren Ränge dann so schlecht, dass es nichts bringt die schonmal gesehen zu haben.
		Oder: gleiche Peptide in den gleichen split
			Bilder: percolator_acv_{}_PeptideTest.png
			Schlechterer Score insgesamt, nonXLs werden eher immer schlechter
			Aber eher ehrlich, vielleicht besser zum generalisieren? 
				auf was soll man aber noch generalisieren? die svm wird gelöscht sobald der split fertig ist?
				ist es hier unehrlich oder besseres training, wenn man der svmgute Beispiele zeigt?
					Beispiele (= Elemente des train set), bei denen Peptide dabei sind, die dann auch im test set sind.
					Die Generalisierung von manchen Peptiden auf alle Pepide fällt der SVM schwer, weil der score sinkt wenn man die Peptide im test set nicht auch im train set zeigt. 
					Idee: Vielleicht sind es nur wenige, bestimmte Peptide (oder Proteine), die besondere Eigenschaften haben und somit schlecht vorhergesagt werden können?
					Oder: Peptide von decoys und peptide von targets sind disjunkt. Vielleicht haben gleiche Peptide gleiche Eigenschaften in manchen der scores, und die SVM kriegt diese Eigenschaften über die false train mit? Und kann somit zwischen decoy und target direkt unterscheiden?
					Zu beachten: nXL werden hier immer schlechter!
	
13.	KFold, nested score benutzen, groupKFold anschauen? https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html
	Statt stratified KFold normales KFold benutzen macht praktisch keinen Unterschied: percolator_acv_{}_KFoldTest.png
	splitting in drei Teile mit KFold implementieren vielleicht eleganter? Nein:
		ixTraining, ixValidate = kf.split(df)	
		aber in zukünftigen Iterationen? Man müsste sich indices speichern, ixTraining nochmal spliten und dann manuell neu zusammensetzen. 
		Das ist nicht besser als wie es jetzt ist.
		Aber mit leave one group out? 
	outer cross validation (outer cv) muss man selbst bauen, weil bei cross_val_predict nur supervised oder unsupervised, aber nicht semi-supervised unterstützt wird.

14.	Balancing:
	Bei inner cross-validation für class_weights "balanced" hinzugefügt. Wird aber praktisch nie ausgewählt. (Stimmt der Gedanke, dass es für SVMs auch gar nicht so wichtig ist, weil da ja nur die support vectors wichtig sind und alle anderen nicht? Nein -> C ist die penalty für Falsch-Klassifizierungen, und die kommen ja trotzdem vor.)
		Wieder rausgemacht, weil es dadurch 33% länger dauert. 
	Genau gleich viele von Tar:Dec oder XL:nXL macht keinen Sinn, weil dann ja Elemente der größeren Gruppe überbleiben. Wo würde man die hintun?
		Möglich wäre, den DataFrame vorher zu filtern, sodass es immer gleich viele gibt und dann proportional zu verteilen. Aber dann hat man viel weniger PSMs -> nicht sinnvoll
	Tar:Dec proportional zum Verhältnis im ganzen DataFrame funktioniert (s. Code-Bilder in results/balancingTest)
	Ergebnisse:
	used balancing: True, max AUCs: [343.2544021400075], min AUCs: [341.2731519815712], median AUCs: 342.3760451913073
	used balancing: False, max AUCs: [343.8859680229368], min AUCs: [342.05103793927975], median AUCs: 342.7538097947935
	Bilder: percolator_acv_MaxMinMedian_balancing={}.png
	
15.	Gemerkt: Experimente machen wenig Sinn, wenn nur den ersten Rang verwenden die allerbesten Ergebnisse bringt. Darum Experimente damit, um neues Standard etablieren zu können:
	Nur die ersten beiden Ränge benutzen macht keinen großen Unterschied. (Bilder: percolator_acv_{}_TopTwoRanks.png, finales AUC: 342.442)
	Vielleicht ein paar Iterationen, um die Ränge neu zu ordnen, dann alle außer ersten rausschmeißen und weiter trainieren? Option: optimalRanking
		-> bestes Ergebnis bisher. Damit neue Tests. Anzahl Iterationen verändern? -> 5,3 ziemlich genau wie 4 <- neuer Standard

	Erneuter Test mit Balancing:
	10 Wiederholungen pro Option (balance in [True, False]):
	p = percolator_experimental(dSlow, idCol, features, suppressLog = True, plotEveryIter = False, optimalRanking = 4, propTarDec = balance, propXLnXL = balance, balancingInner = balance, specialGrid = True, rankOption = False, balancingOuter = balance)
	used balancing: False, max AUCs: [349.1406103405052, 53.62776494990441, 294.41672772316707], min AUCs: [347.0401814390844, 54.339546009885346, 291.51140599645385], median AUCs: [347.69301852109606, 53.775556995129946, 292.85960639082884]
	used balancing: True, max AUCs: [348.65198905387354, 53.85216435170418, 293.90713490741655], min AUCs: [347.47934498739266, 53.76340540673226, 293.1242788154449], median AUCs: [348.1949527179018, 53.83653035743281, 293.58185177459325]
	Bilder: percolator_acv_MaxMinMedian_optimalRanking_balancing=False.png

	AB HIER: NEUER STANDARD: optimalRanking = True, specialGrid = True, balancing = True, rankOption = False

	specialGrid sollte theoretisch keinen Unterschied machen, da man ja nach dem score (= Entfernung von der Hyperebene) sortiert und dann nach
	geschätzem q-value. Ob man die Hyperebene jetzt mehr in die Richtung der targets schiebt (mit größerem C für decoys, da dadurch decoys auf 
	der falschen Seite bestraft werden und die Ebene so gelegt wird dass weniger decoys falsch klassifiziert werden) oder nicht, sollte also 
	keinen unterschied machen.

	https://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html

TO-DO: LeaveOneGroupOut einbauen (s. Codeschnipsel oben)
Ideen für Zukunft:
	Welche mächtigeren Klassifikatoren wären sinnvoll? z.b. auch non-linear svms? Ein mächtigerer Klassifikator (hat bei Timo leider nicht gut funktioniert bzw. hat zu stark overfitted) könnte evtl. funktionieren falls wir zusätzliche Bedingungen beim lernen einfügen: https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints
	Ist das nicht das gleiche wie was ich mache? Trainieren, neu ordnen, trainieren? Boosting der linearen SVM: https://www.datacamp.com/community/tutorials/adaboost-classifier-python
	was ist feature selection: https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py
		Jeder Datensatz zB. unterschiedliche parameter


