PSM = Peptide Spectrum Matches
# Frage: Warum generiert man nicht mehrere decoy-databases (shuffled, reversed...), damit nicht jeder zweite falsche Eintrag ein decoy ist, sondern jeder dritte, vierte, fünfte....?
# Antwort: Sehr sehr aufwändig, besonders bei cross-links. Wenn ich ne Idee habe kann ich die Daten dazu bekommen. Viele decoys bedeuten dass auch hohe scores random generiert werden.
# Frage: Die Aufteilung auf 3 subsets für cross-validation nur mit PSMs, die eh fürs Training verwendet werden oder mit allen und dann werden daraus die benötigten PSMs ausgewählt (momentan implementiert)?
# Antwort: Gute Frage, überlegen wir uns nochmal. Überlegung wert, gleichmäßiger samplen oä
# Frage: Warum kommt immer ein anderes Ergebnis, wenn man öfter die Funktion decision_function anwendet ohne neu zu fitten? Werden den PSMs nicht immer die gleichen scores gegeben? Code:
df['test_score'] = W.decision_function(X)
calcQ(df,'test_score')
print(len(df[df['q-val'] <= 0.01]))

# Frage: Was bringt normalisierung nach unit norm? -> Ziemliche Verbesserung der Trennleistung wie es aussieht...

1.	Mit Art der Daten vertraut machen, parsen, FDR und q-value berechnen, Pseudo-ROCs zeichnen (Ordner 'Erste Aufgabe')
2.	Richtige Daten mit XL bekommen, Skript aus 'Erste Aufgabe' anwenden, Pseudo-ROCs für XL und nicht-XL zeichnen.
3.	GitHub anlegen
4.	Rang-Spalte hinzufügen und jeweils nur Rang 1 für ROCs benutzen
a.	Vorher (alle Einträge mit entspr. Q-Value, fast method, orange: XL, blue: non-XL): ROC_fast_all.jpg
b.	Nachher (nur Rang-1-Einträge, fast method, orange: XL, blue: non-XL): ROC_fast_rank1only.jpg
c. 	Non-cross-links sehen gleich aus, Cross-links haben gleiche Form aber Anzahl ist bei allen Ranks bis q = 0.05 bei ca. 6000, nur bei Rank 1 nur ca. 4000
5.	Einlesen in lineare ML-Ansätze und ML-package scikit Learn
a. 	LDA: Geht von Normalverteilungen mit unterschiedlichem Erwartungswert und/oder Varianz der einzelnen Scores für jede Klasse aus. Das passt nicht wirklich auf unser Beispiel, da (zumindest die meisten) scores nicht normalverteilt sind (oder?). Ergebnis: lda_compared_to_NuXLscore.png
b. 	Ladder score ist etwas normalverteilt, mit einem deutlich unterschiedlichen Erwartungswert zwischen den wahren und den falschen Trainigsdaten. Nur den zu verwenden macht das Training (natürlich?) nicht besser, wieviel schlechter es ist müsste man noch mehr rausfinden. Ergebnis: lda_of_ladder_score.png
c.	Logistic regression: Trennt ähnlich wie NuXL score. Warum? Wie wird NuXL score zusammengesetzt? Ergebnis: lr_compared_to_NuXLscore.png
d.	SVM: Trennt ähnlich wie NuXL score, wird von percolator benutzt, versucht die Grenze so zu legen, dass der Abstand der Grenze zu Objekten der Klasse maximal ist, ohne zu viele zu wichtige Datenpunkte als Ausreißer zu klassifizieren. Ergebnis: svm_compared_to_NuXLscore.png
6.	Struktur des Notebook verändern, dass man die Module beliebig zusammensetzen kann
7.	Percolator-Algorithmus nachbauen 
Notes:	Seltsames Verhalten der Scores der SVM, siehe Linear_ML.ipynb letzte Zelle. Vielleicht müssen die Scores nochmal transformiert werden? Darum benutze ich allerdings predict_proba bzw. eine lineare SVM, bei der gibt es die Probleme nicht.
	Option 'dual = False' hat es signifikant schneller werden lassen, sodass das Lernen fertig wird (konvergiert) und die Ergebnisse tatsächlich besser sind als der NuXL:score
	Auch die Pseudo-ROC-Kurven wenn man nach dem NuXL:score geht sehen unterschiedlich aus, da im Percolator-Algorithmus zufällig die Hälfte der Decoys entfernt wird.
	Experiment:
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = : 5569
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = balanced: 5560
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = : 5351
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = balanced: 5185
		
		Genauere Untersuchung zu den Klassen Optionen mit den Eckwerten von je 10 Ausführungen (q <= 0.01, rank = 1):
		class Option: , max PSMs: 5285, min PSMs: 4413, median PSMs: 5203.0
		class Option: balanced, max PSMs: 5261, min PSMs: 5067, median PSMs: 5179.0

		Genauere Untersuchung dazu, nur Rang 1 fürs Training zu benutzen, zum Scoring aber alle zu nehmen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5111, min PSMs: 4876, median PSMs: 4987.5
		useRankOneOnly: False, max PSMs: 5333, min PSMs: 4320, median PSMs: 4956.0

		Genauere Untersuchung dazu, nur Rang 1 PSMs für Training und Scoring zu benutzen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5669, min PSMs: 5011, median PSMs: 5569.0
		useRankOneOnly: False, max PSMs: 5380, min PSMs: 4364, median PSMs: 5238.0
		Mögliche Gründe: PSMs mit q <= 0.01 aber Rang > 1 (also eher schlecht) werden ausgeschlossen. Es gibt weniger schlechte PSMs im df die man falsch klassifizieren kann?

		Vergleich: mit NuXL:score mit q <= 0.01 findet man 4067 PSMs.
		Bilder jeweils im 'results' Ordner.
8.	Cross-Validation einbauen
9.	XL-q-value berechnen: Q values nur mit den klassenmembern berechnen (extra spalte).
		Iterationen-plots für XL und nicht XL getrennt. 3 plots: XL, nicht-XL, beides zusammen, plus AUC in Legende
		für allgmeine ROC kurven auch klassen q values benutzen
10. 	Wenn es in den letzten 4 (standard) Iterationen schlechter wird, wird letztes bestes genommen
11.	features nach unit norm normalisieren. d.h. min jedes features wird 0 und maximum wird 1.
12.	Experimente:
	Normal: percolator_acv_{}.png
	Ränge mit einbeziehen:
		Wenn auf Rang 1 target : target, wenn auf rang 1 decoy, alle decoy? "RankOption = True": uses all PSMs of species, of which the first-ranked PSM is a decoy, as negative training examples, and no longer as positive training examples.
		Ergebnisse bleiben etwa gleich. decoy trainingsbeispiele: 42592 decoys + 11200 = 53792, weniger pos. Trainingsbeispiele: 5498 - 18 = 5480. Bilder: percolator_acv_{}_rankOptionTrue.png
		
		Während training Rang 2+ target als decoy behandeln -> Jede Iteration schlechter, ob bei pos train set nur Rang 1 betrachtet wurde (Bilder: percolator_acv_rankTest_{}.png) oder nicht.
		zum scoring aber immer wieder wie original!
		
		!!!AB HIER: rankOption = True neues normal!!!

	ScanNr Versuche:
		Sehr ähnliche Peptide auf verschiedene splits verteilen führt zu gleichen PSMs in training und test set. Abhilfe: nach scanNr sortieren und dann splitten (nicht mehr random, abhilfe: gleiche scanNr beim aufteilen als 1 Element behandeln -> alle gleichen scanNr kommen in gleichen Split)
			Momentan noch hässlich implementiert mit immer neuen DataFrames -> müsste man ändern. 
			Ergebnisse nicht viel anders als sonst. Bilder: percolator_acv_{}_ScanNrTest.png.
		Oder: gleiche Peptide in den gleichen split
13.	KFold, nested score benutzen, groupKFold anschauen? https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html
14.	Ein mächtigerer Klassifikator (hat bei Timo leider nicht gut funktioniert bzw. hat zu stark overfitted) könnte evtl. funktionieren falls wir zusätzliche Bedingungen beim lernen einfügen: https://scikit-learn.org/stable/modules/ensemble.html#monotonic-constraints
15.	Boosting der linearen SVM: https://www.datacamp.com/community/tutorials/adaboost-classifier-python
16.	feature selection: https://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py

