1.	Mit Art der Daten vertraut machen, parsen, FDR und q-value berechnen, Pseudo-ROCs zeichnen (Ordner 'Erste Aufgabe')
2.	Richtige Daten mit XL bekommen, Skript aus 'Erste Aufgabe' anwenden, Pseudo-ROCs für XL und nicht-XL zeichnen.
3.	GitHub anlegen
4.	Rang-Spalte hinzufügen und jeweils nur Rang 1 für ROCs benutzen
a.	Vorher (alle Einträge mit entspr. Q-Value, fast method, orange: XL, blue: non-XL): ROC_fast_all.jpg
b.	Nachher (nur Rang-1-Einträge, fast method, orange: XL, blue: non-XL): ROC_fast_rank1only.jpg
c. 	Non-cross-links sehen gleich aus, Cross-links haben gleiche Form aber Anzahl ist bei allen Ranks bis q = 0.05 bei ca. 6000, nur bei Rank 1 nur ca. 4000
5.	Einlesen in lineare ML-Ansätze und ML-package scikit Learn
a. 	LDA: Geht von Normalverteilungen mit unterschiedlichem Erwartungswert und/oder Varianz der einzelnen Scores für jede Klasse aus. Das passt nicht wirklich auf unser Beispiel, da (zumindest die meisten) scores nicht normalverteilt sind (oder?). Ergebnis: lda_compared_to_NuXLscore.png
b. 	Ladder score ist etwas normalverteilt, mit einem deutlich unterschiedlichen Erwartungswert zwischen den wahren und den falschen Trainigsdaten. Nur den zu verwenden macht das Training (natürlich?) nicht besser, wieviel schlechter es ist müsste man noch mehr rausfinden. Ergebnis: lda_of_ladder_score.png
c.	Logistic regression: Trennt ähnlich wie NuXL score. Warum? Wie wird NuXL score zusammengesetzt? Ergebnis: lr_compared_to_NuXLscore.png
d.	SVM: Trennt ähnlich wie NuXL score, wird von percolator benutzt, versucht die Grenze so zu legen, dass der Abstand der Grenze zu Objekten der Klasse maximal ist, ohne zu viele zu wichtige Datenpunkte als Ausreißer zu klassifizieren. Ergebnis: svm_compared_to_NuXLscore.png
6.	Struktur des Notebook verändern, dass man die Module beliebig zusammensetzen kann
7.	Percolator-Algorithmus nachbauen 
8.	Cross-Validation einbauen
