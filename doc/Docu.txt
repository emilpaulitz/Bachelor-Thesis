PSM = Peptide Spectrum Matches
# Frage: Warum generiert man nicht mehrere decoy-databases (shuffled, reversed...), damit nicht jeder zweite falsche Eintrag ein decoy ist, sondern jeder dritte, vierte, fünfte....?
# Antwort: Sehr sehr aufwändig, besonders bei cross-links. Wenn ich ne Idee habe kann ich die Daten dazu bekommen. Viele decoys bedeuten dass auch hohe scores random generiert werden.
# Frage: Die Aufteilung auf 3 subsets für cross-validation nur mit PSMs, die eh fürs Training verwendet werden oder mit allen und dann werden daraus die benötigten PSMs ausgewählt (momentan implementiert)?
# Antwort: Gute Frage, überlegen wir uns nochmal. Überlegung wert, gleichmäßiger samplen oä
# Frage: Warum kommt immer ein anderes Ergebnis, wenn man öfter die Funktion decision_function anwendet ohne neu zu fitten? Werden den PSMs nicht immer die gleichen scores gegeben? Code:
df['test_score'] = W.decision_function(X)
calcQ(df,'test_score')
print(len(df[df['q-val'] <= 0.01]))

1.	Mit Art der Daten vertraut machen, parsen, FDR und q-value berechnen, Pseudo-ROCs zeichnen (Ordner 'Erste Aufgabe')
2.	Richtige Daten mit XL bekommen, Skript aus 'Erste Aufgabe' anwenden, Pseudo-ROCs für XL und nicht-XL zeichnen.
3.	GitHub anlegen
4.	Rang-Spalte hinzufügen und jeweils nur Rang 1 für ROCs benutzen
a.	Vorher (alle Einträge mit entspr. Q-Value, fast method, orange: XL, blue: non-XL): ROC_fast_all.jpg
b.	Nachher (nur Rang-1-Einträge, fast method, orange: XL, blue: non-XL): ROC_fast_rank1only.jpg
c. 	Non-cross-links sehen gleich aus, Cross-links haben gleiche Form aber Anzahl ist bei allen Ranks bis q = 0.05 bei ca. 6000, nur bei Rank 1 nur ca. 4000
5.	Einlesen in lineare ML-Ansätze und ML-package scikit Learn
a. 	LDA: Geht von Normalverteilungen mit unterschiedlichem Erwartungswert und/oder Varianz der einzelnen Scores für jede Klasse aus. Das passt nicht wirklich auf unser Beispiel, da (zumindest die meisten) scores nicht normalverteilt sind (oder?). Ergebnis: lda_compared_to_NuXLscore.png
b. 	Ladder score ist etwas normalverteilt, mit einem deutlich unterschiedlichen Erwartungswert zwischen den wahren und den falschen Trainigsdaten. Nur den zu verwenden macht das Training (natürlich?) nicht besser, wieviel schlechter es ist müsste man noch mehr rausfinden. Ergebnis: lda_of_ladder_score.png
c.	Logistic regression: Trennt ähnlich wie NuXL score. Warum? Wie wird NuXL score zusammengesetzt? Ergebnis: lr_compared_to_NuXLscore.png
d.	SVM: Trennt ähnlich wie NuXL score, wird von percolator benutzt, versucht die Grenze so zu legen, dass der Abstand der Grenze zu Objekten der Klasse maximal ist, ohne zu viele zu wichtige Datenpunkte als Ausreißer zu klassifizieren. Ergebnis: svm_compared_to_NuXLscore.png
6.	Struktur des Notebook verändern, dass man die Module beliebig zusammensetzen kann
7.	Percolator-Algorithmus nachbauen 
Notes:	Seltsames Verhalten der Scores der SVM, siehe Linear_ML.ipynb letzte Zelle. Vielleicht müssen die Scores nochmal transformiert werden? Darum benutze ich allerdings predict_proba bzw. eine lineare SVM, bei der gibt es die Probleme nicht.
	Option 'dual = False' hat es signifikant schneller werden lassen, sodass das Lernen fertig wird (konvergiert) und die Ergebnisse tatsächlich besser sind als der NuXL:score
	Auch die Pseudo-ROC-Kurven wenn man nach dem NuXL:score geht sehen unterschiedlich aus, da im Percolator-Algorithmus zufällig die Hälfte der Decoys entfernt wird.
	Experiment:
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = : 5569
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = True and classes option = balanced: 5560
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = : 5351
		Number of PSMs for q <= 0.01, rank = 1, using rank 1 only = False and classes option = balanced: 5185
		
		Genauere Untersuchung zu den Klassen Optionen mit den Eckwerten von je 10 Ausführungen (q <= 0.01, rank = 1):
		class Option: , max PSMs: 5285, min PSMs: 4413, median PSMs: 5203.0
		class Option: balanced, max PSMs: 5261, min PSMs: 5067, median PSMs: 5179.0

		Genauere Untersuchung dazu, nur Rang 1 fürs Training zu benutzen, zum Scoring aber alle zu nehmen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5111, min PSMs: 4876, median PSMs: 4987.5
		useRankOneOnly: False, max PSMs: 5333, min PSMs: 4320, median PSMs: 4956.0

		Genauere Untersuchung dazu, nur Rang 1 PSMs für Training und Scoring zu benutzen (auch je 10 Ausführungen):
		useRankOneOnly: True, max PSMs: 5669, min PSMs: 5011, median PSMs: 5569.0
		useRankOneOnly: False, max PSMs: 5380, min PSMs: 4364, median PSMs: 5238.0
		Mögliche Gründe: PSMs mit q <= 0.01 aber Rang > 1 (also eher schlecht) werden ausgeschlossen. Es gibt weniger schlechte PSMs im df die man falsch klassifizieren kann?

		Vergleich: mit NuXL:score mit q <= 0.01 findet man 4067 PSMs.
		Bilder jeweils im 'results' Ordner.
8.	Cross-Validation einbauen
9.	Ränge mit einbeziehen:
		Wenn auf Rang 1 target : target, wenn auf rang 1 decoy, alle decoy? "RankOption = True": uses all PSMs of species, of which the first-ranked PSM is a decoy, as negative training examples, and no longer as positive training examples.
		Ergebnisse bleiben etwa gleich, bei 1 Versuch ging Konvergieren etwas schneller. decoy trainingsbeispiele: 42592 decoys + 11200 = 53792, weniger pos. Trainingsbeispiele: 5498 - 18 = 5480. Bilder: percolator_acv_{}_rankOptionTrue.png
		während training Rang 2+ target als decoy behandeln? -> Jede Iteration schlechter, ob bei pos train set nur Rang 1 betrachtet wurde (Bilder: percolator_acv_rankTest_{}.png) oder nicht.
		zum scoring aber immer wieder wie original!
10.	XL-q-value berechnen: Q values nur mit den klassenmembern berechnen (extra spalte).
		Iterationen-plots für XL und nicht XL getrennt. 3 plots: XL, nicht-XL, beides zusammen
		Plus metrik (AUC?)
		für ROC kurven auch klassen q values benutzen
11. 	Wenn es in den Iterationen schlechter wird, vllt letztes bestes nehmen?